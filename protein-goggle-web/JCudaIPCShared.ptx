//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19324574
// Cuda compilation tools, release 7.0, V7.0.27
// Based on LLVM 3.4svn
//

.version 4.2
.target sm_20
.address_size 64

	// .globl	ipc
.extern .func  (.param .b64 func_retval0) malloc
(
	.param .b64 malloc_param_0
)
;

.visible .entry ipc(
	.param .u64 ipc_param_0,
	.param .u64 ipc_param_1,
	.param .u64 ipc_param_2,
	.param .u64 ipc_param_3,
	.param .u64 ipc_param_4,
	.param .u64 ipc_param_5,
	.param .u64 ipc_param_6,
	.param .u64 ipc_param_7,
	.param .u64 ipc_param_8,
	.param .u64 ipc_param_9,
	.param .u64 ipc_param_10,
	.param .u64 ipc_param_11,
	.param .u64 ipc_param_12,
	.param .u64 ipc_param_13
)
{
	.reg .pred 	%p<23>;
	.reg .s32 	%r<85>;
	.reg .f64 	%fd<34>;
	.reg .s64 	%rd<162>;


	ld.param.u64 	%rd84, [ipc_param_0];
	ld.param.u64 	%rd85, [ipc_param_1];
	ld.param.u64 	%rd86, [ipc_param_2];
	ld.param.u64 	%rd88, [ipc_param_4];
	ld.param.u64 	%rd89, [ipc_param_5];
	ld.param.u64 	%rd90, [ipc_param_6];
	ld.param.u64 	%rd92, [ipc_param_8];
	ld.param.u64 	%rd93, [ipc_param_9];
	ld.param.u64 	%rd97, [ipc_param_10];
	ld.param.u64 	%rd94, [ipc_param_11];
	ld.param.u64 	%rd95, [ipc_param_12];
	ld.param.u64 	%rd96, [ipc_param_13];
	cvta.to.global.u64 	%rd98, %rd94;
	cvta.to.global.u64 	%rd99, %rd97;
	ldu.global.u32 	%r1, [%rd99+4];
	ldu.global.u32 	%r2, [%rd99+8];
	ldu.global.u32 	%r3, [%rd99+12];
	ldu.global.u32 	%r4, [%rd99+16];
	ldu.global.u32 	%r5, [%rd99];
	mul.wide.s32 	%rd100, %r5, 8;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd100;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd1, [retval0+0];
	
	//{
	}// Callseq End 0
	mul.wide.s32 	%rd101, %r1, 8;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd101;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd2, [retval0+0];
	
	//{
	}// Callseq End 1
	mul.wide.s32 	%rd102, %r2, 8;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd102;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd3, [retval0+0];
	
	//{
	}// Callseq End 2
	mul.wide.s32 	%rd103, %r3, 8;
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd103;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd4, [retval0+0];
	
	//{
	}// Callseq End 3
	mul.wide.s32 	%rd104, %r4, 8;
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd104;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd5, [retval0+0];
	
	//{
	}// Callseq End 4
	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd100;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd6, [retval0+0];
	
	//{
	}// Callseq End 5
	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd101;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd7, [retval0+0];
	
	//{
	}// Callseq End 6
	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd102;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd8, [retval0+0];
	
	//{
	}// Callseq End 7
	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd103;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd9, [retval0+0];
	
	//{
	}// Callseq End 8
	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd104;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd10, [retval0+0];
	
	//{
	}// Callseq End 9
	mov.u32 	%r47, %ctaid.x;
	mul.lo.s32 	%r48, %r47, 5;
	mul.wide.s32 	%rd105, %r48, 4;
	add.s64 	%rd106, %rd98, %rd105;
	ld.global.u32 	%r6, [%rd106+4];
	ld.global.u32 	%r7, [%rd106+8];
	ld.global.u32 	%r8, [%rd106+12];
	ld.global.u32 	%r9, [%rd106+16];
	setp.eq.s32	%p1, %r5, 0;
	@%p1 bra 	BB0_3;

	ld.global.u32 	%r51, [%rd106];
	add.s32 	%r52, %r51, -1;
	neg.s32 	%r74, %r5;
	mul.lo.s32 	%r53, %r5, %r52;
	mul.wide.s32 	%rd11, %r53, 8;
	cvta.to.global.u64 	%rd136, %rd84;
	cvta.to.global.u64 	%rd137, %rd89;
	mov.u64 	%rd138, %rd1;
	mov.u64 	%rd139, %rd6;

BB0_2:
	mov.u64 	%rd17, %rd139;
	mov.u64 	%rd16, %rd138;
	add.s64 	%rd110, %rd136, %rd11;
	ld.global.f64 	%fd4, [%rd110];
	st.f64 	[%rd16], %fd4;
	add.s64 	%rd111, %rd137, %rd11;
	ld.global.f64 	%fd5, [%rd111];
	st.f64 	[%rd17], %fd5;
	add.s64 	%rd18, %rd17, 8;
	add.s64 	%rd19, %rd16, 8;
	add.s64 	%rd137, %rd137, 8;
	add.s64 	%rd136, %rd136, 8;
	add.s32 	%r74, %r74, 1;
	setp.ne.s32	%p2, %r74, 0;
	mov.u64 	%rd138, %rd19;
	mov.u64 	%rd139, %rd18;
	@%p2 bra 	BB0_2;

BB0_3:
	setp.eq.s32	%p3, %r1, 0;
	@%p3 bra 	BB0_9;

	neg.s32 	%r75, %r1;
	cvta.to.global.u64 	%rd140, %rd85;
	cvta.to.global.u64 	%rd141, %rd90;
	mov.u64 	%rd142, %rd2;
	mov.u64 	%rd143, %rd7;

BB0_5:
	mov.u64 	%rd27, %rd143;
	mov.u64 	%rd26, %rd142;
	setp.gt.s32	%p4, %r6, 0;
	@%p4 bra 	BB0_7;
	bra.uni 	BB0_6;

BB0_7:
	add.s32 	%r54, %r6, -1;
	mul.lo.s32 	%r55, %r1, %r54;
	mul.wide.s32 	%rd113, %r55, 8;
	add.s64 	%rd114, %rd140, %rd113;
	ld.global.f64 	%fd7, [%rd114];
	st.f64 	[%rd26], %fd7;
	add.s64 	%rd115, %rd141, %rd113;
	ld.global.f64 	%fd33, [%rd115];
	bra.uni 	BB0_8;

BB0_6:
	mov.u64 	%rd112, 0;
	st.u64 	[%rd26], %rd112;
	mov.f64 	%fd33, 0d3FF0000000000000;

BB0_8:
	st.f64 	[%rd27], %fd33;
	add.s64 	%rd28, %rd27, 8;
	add.s64 	%rd29, %rd26, 8;
	add.s64 	%rd141, %rd141, 8;
	add.s64 	%rd140, %rd140, 8;
	add.s32 	%r75, %r75, 1;
	setp.ne.s32	%p5, %r75, 0;
	mov.u64 	%rd142, %rd29;
	mov.u64 	%rd143, %rd28;
	@%p5 bra 	BB0_5;

BB0_9:
	setp.eq.s32	%p6, %r2, 0;
	@%p6 bra 	BB0_12;

	ld.param.u64 	%rd128, [ipc_param_7];
	add.s32 	%r56, %r7, -1;
	neg.s32 	%r76, %r2;
	mul.lo.s32 	%r57, %r2, %r56;
	mul.wide.s32 	%rd32, %r57, 8;
	cvta.to.global.u64 	%rd144, %rd86;
	cvta.to.global.u64 	%rd145, %rd128;
	mov.u64 	%rd146, %rd3;
	mov.u64 	%rd147, %rd8;

BB0_11:
	mov.u64 	%rd38, %rd147;
	mov.u64 	%rd37, %rd146;
	add.s64 	%rd116, %rd144, %rd32;
	ld.global.f64 	%fd8, [%rd116];
	st.f64 	[%rd37], %fd8;
	add.s64 	%rd117, %rd145, %rd32;
	ld.global.f64 	%fd9, [%rd117];
	st.f64 	[%rd38], %fd9;
	add.s64 	%rd39, %rd38, 8;
	add.s64 	%rd40, %rd37, 8;
	add.s64 	%rd145, %rd145, 8;
	add.s64 	%rd144, %rd144, 8;
	add.s32 	%r76, %r76, 1;
	setp.ne.s32	%p7, %r76, 0;
	mov.u64 	%rd146, %rd40;
	mov.u64 	%rd147, %rd39;
	@%p7 bra 	BB0_11;

BB0_12:
	setp.eq.s32	%p8, %r3, 0;
	@%p8 bra 	BB0_15;

	ld.param.u64 	%rd129, [ipc_param_3];
	add.s32 	%r58, %r8, -1;
	neg.s32 	%r77, %r3;
	mul.lo.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd43, %r59, 8;
	cvta.to.global.u64 	%rd148, %rd129;
	cvta.to.global.u64 	%rd149, %rd92;
	mov.u64 	%rd150, %rd4;
	mov.u64 	%rd151, %rd9;

BB0_14:
	mov.u64 	%rd49, %rd151;
	mov.u64 	%rd48, %rd150;
	add.s64 	%rd118, %rd148, %rd43;
	ld.global.f64 	%fd10, [%rd118];
	st.f64 	[%rd48], %fd10;
	add.s64 	%rd119, %rd149, %rd43;
	ld.global.f64 	%fd11, [%rd119];
	st.f64 	[%rd49], %fd11;
	add.s64 	%rd50, %rd49, 8;
	add.s64 	%rd51, %rd48, 8;
	add.s64 	%rd149, %rd149, 8;
	add.s64 	%rd148, %rd148, 8;
	add.s32 	%r77, %r77, 1;
	setp.ne.s32	%p9, %r77, 0;
	mov.u64 	%rd150, %rd51;
	mov.u64 	%rd151, %rd50;
	@%p9 bra 	BB0_14;

BB0_15:
	setp.eq.s32	%p10, %r4, 0;
	@%p10 bra 	BB0_18;

	add.s32 	%r60, %r9, -1;
	neg.s32 	%r78, %r4;
	mul.lo.s32 	%r61, %r4, %r60;
	mul.wide.s32 	%rd54, %r61, 8;
	cvta.to.global.u64 	%rd152, %rd88;
	cvta.to.global.u64 	%rd153, %rd93;
	mov.u64 	%rd158, %rd5;
	mov.u64 	%rd161, %rd10;

BB0_17:
	add.s64 	%rd120, %rd152, %rd54;
	ld.global.f64 	%fd12, [%rd120];
	st.f64 	[%rd158], %fd12;
	add.s64 	%rd121, %rd153, %rd54;
	ld.global.f64 	%fd13, [%rd121];
	st.f64 	[%rd161], %fd13;
	add.s64 	%rd161, %rd161, 8;
	add.s64 	%rd158, %rd158, 8;
	add.s64 	%rd153, %rd153, 8;
	add.s64 	%rd152, %rd152, 8;
	add.s32 	%r78, %r78, 1;
	setp.ne.s32	%p11, %r78, 0;
	@%p11 bra 	BB0_17;

BB0_18:
	@%p1 bra 	BB0_33;

	mov.u32 	%r73, %ctaid.x;
	setp.gt.s32	%p13, %r6, 0;
	selp.f64	%fd3, 0d3F667847397A7511, 0d3F61F9D29461F741, %p13;
	mul.lo.s32 	%r65, %r73, %r1;
	mul.lo.s32 	%r66, %r65, %r5;
	mul.lo.s32 	%r67, %r66, %r2;
	mul.lo.s32 	%r68, %r67, %r3;
	mul.lo.s32 	%r25, %r68, %r4;
	mov.u32 	%r84, 0;
	mov.u32 	%r79, %r84;
	cvta.to.global.u64 	%rd74, %rd95;
	cvta.to.global.u64 	%rd75, %rd96;

BB0_20:
	@%p3 bra 	BB0_32;

	mul.wide.s32 	%rd122, %r79, 8;
	add.s64 	%rd66, %rd6, %rd122;
	mov.u32 	%r80, 0;

BB0_22:
	@%p6 bra 	BB0_31;

	mul.wide.s32 	%rd123, %r80, 8;
	add.s64 	%rd68, %rd7, %rd123;
	mov.u32 	%r81, 0;

BB0_24:
	@%p8 bra 	BB0_30;

	mul.wide.s32 	%rd124, %r81, 8;
	add.s64 	%rd70, %rd8, %rd124;
	mov.u32 	%r82, 0;

BB0_26:
	@%p10 bra 	BB0_29;

	mul.wide.s32 	%rd125, %r82, 8;
	add.s64 	%rd71, %rd4, %rd125;
	add.s64 	%rd72, %rd9, %rd125;
	add.s32 	%r72, %r25, %r84;
	mul.wide.s32 	%rd73, %r72, 8;
	neg.s32 	%r83, %r4;
	mov.u64 	%rd154, %rd74;
	mov.u64 	%rd155, %rd75;
	mov.u64 	%rd157, %rd5;
	mov.u64 	%rd160, %rd10;

BB0_28:
	mov.u64 	%rd79, %rd160;
	mov.u64 	%rd78, %rd157;
	mov.u64 	%rd77, %rd155;
	mov.u64 	%rd76, %rd154;
	mul.wide.s32 	%rd135, %r81, 8;
	add.s64 	%rd134, %rd3, %rd135;
	mul.wide.s32 	%rd133, %r79, 8;
	add.s64 	%rd132, %rd1, %rd133;
	mul.wide.s32 	%rd131, %r80, 8;
	add.s64 	%rd130, %rd2, %rd131;
	ld.f64 	%fd14, [%rd130];
	ld.f64 	%fd15, [%rd132];
	add.f64 	%fd16, %fd15, %fd14;
	ld.f64 	%fd17, [%rd134];
	add.f64 	%fd18, %fd16, %fd17;
	ld.f64 	%fd19, [%rd71];
	add.f64 	%fd20, %fd18, %fd19;
	ld.f64 	%fd21, [%rd78];
	add.f64 	%fd22, %fd20, %fd21;
	add.f64 	%fd23, %fd3, %fd22;
	add.s64 	%rd126, %rd76, %rd73;
	st.global.f64 	[%rd126], %fd23;
	ld.f64 	%fd24, [%rd68];
	ld.f64 	%fd25, [%rd66];
	mul.f64 	%fd26, %fd25, %fd24;
	ld.f64 	%fd27, [%rd70];
	mul.f64 	%fd28, %fd26, %fd27;
	ld.f64 	%fd29, [%rd72];
	mul.f64 	%fd30, %fd28, %fd29;
	ld.f64 	%fd31, [%rd79];
	mul.f64 	%fd32, %fd30, %fd31;
	add.s64 	%rd127, %rd77, %rd73;
	st.global.f64 	[%rd127], %fd32;
	add.s32 	%r84, %r84, 1;
	add.s64 	%rd80, %rd79, 8;
	add.s64 	%rd81, %rd78, 8;
	add.s64 	%rd82, %rd77, 8;
	add.s64 	%rd83, %rd76, 8;
	add.s32 	%r83, %r83, 1;
	setp.ne.s32	%p18, %r83, 0;
	mov.u64 	%rd154, %rd83;
	mov.u64 	%rd155, %rd82;
	mov.u64 	%rd157, %rd81;
	mov.u64 	%rd160, %rd80;
	@%p18 bra 	BB0_28;

BB0_29:
	add.s32 	%r82, %r82, 1;
	setp.ne.s32	%p19, %r82, %r3;
	@%p19 bra 	BB0_26;

BB0_30:
	add.s32 	%r81, %r81, 1;
	setp.ne.s32	%p20, %r81, %r2;
	@%p20 bra 	BB0_24;

BB0_31:
	add.s32 	%r80, %r80, 1;
	setp.ne.s32	%p21, %r80, %r1;
	@%p21 bra 	BB0_22;

BB0_32:
	add.s32 	%r79, %r79, 1;
	setp.ne.s32	%p22, %r79, %r5;
	@%p22 bra 	BB0_20;

BB0_33:
	ret;
}


